# Prompt改进说明

## 已实现的改进

根据建议，我们已经实现了以下改进：

### 1. 增强Prompt模板

#### schedule_daily.txt
- ✅ 添加了**CRITICAL INSTRUCTIONS**部分
- ✅ 明确禁止添加解释、注释、表格、Markdown格式
- ✅ 要求立即从第一行开始输出格式化的内容
- ✅ 提供了清晰的示例格式

#### schedule_decompose.txt
- ✅ 添加了**CRITICAL INSTRUCTIONS**部分
- ✅ 明确禁止添加"Answer:"等前言
- ✅ 要求立即从"1) "开始输出
- ✅ 强调即使发现冲突也要输出格式化的内容

### 2. 增加超时时间

- ✅ 将`config.json`中的`timeout`从300秒增加到**600秒**（10分钟）
- ✅ 这应该能够处理更复杂的推理任务

### 3. 增强解析逻辑

#### schedule_decompose解析改进
- ✅ 能够从解释性文本中提取格式化的部分
- ✅ 支持多种提取模式：
  - 查找`<Subtasks>`标签
  - 查找以"1) "开头的编号列表
  - 提取所有以数字开头的行
- ✅ 更灵活的正则表达式模式匹配

#### 重试机制改进
- ✅ 在重试时自动添加更严格的格式指令
- ✅ 针对`schedule_daily`和`schedule_decompose`添加特定提醒

## 改进效果

### 预期改进

1. **减少格式不匹配错误**：
   - 更严格的Prompt应该让LLM更倾向于输出格式化内容
   - 即使LLM添加了解释，解析逻辑也能提取格式化部分

2. **减少超时错误**：
   - 600秒的超时应该足够处理复杂的推理任务
   - 减少因超时导致的重试

3. **提高解析成功率**：
   - 增强的解析逻辑能够处理更多格式变体
   - 即使LLM不完全遵循格式，也能提取有用信息

## 使用建议

### 如果问题仍然存在

1. **检查LLM响应**：
   - 查看日志中的"Response preview"
   - 确认LLM是否遵循了新的格式要求

2. **进一步增加超时**：
   - 如果600秒仍然不够，可以增加到900秒（15分钟）
   - 修改`config.json`中的`timeout`值

3. **考虑使用更小的模型**：
   - 更小的模型可能更"听话"，更严格遵循格式
   - 例如：`qwen2.5:3b-q4_K_M`

4. **调整temperature**：
   - 降低temperature可以减少随机性
   - 使输出更可预测

## 配置示例

### config.json
```json
{
    "agent": {
        "think": {
            "llm": {
                "timeout": 600  // 已从300增加到600
            }
        }
    }
}
```

### 如果需要进一步增加超时
```json
{
    "agent": {
        "think": {
            "llm": {
                "timeout": 900  // 15分钟
            }
        }
    }
}
```

## 总结

所有建议的改进都已实现：
- ✅ Prompt模板增强
- ✅ 超时时间增加
- ✅ 解析逻辑增强
- ✅ 重试机制改进

现在应该能够显著减少LLM解析错误。如果问题仍然存在，请检查日志中的具体错误信息，我们可以进一步调整。

